# Deep-XAI-Stable Configuration
# Stablecoin Price Prediction with Explainability

# --- Data Collection ---
data:
  stablecoins:
    - symbol: "USDT/USDT"
      name: "Tether"
      coingecko_id: "tether"
      chain: "ethereum"
    - symbol: "USDC/USDT"
      name: "USD Coin"
      coingecko_id: "usd-coin"
      chain: "ethereum"
    - symbol: "DAI/USDT"
      name: "Dai"
      coingecko_id: "dai"
      chain: "ethereum"

  time_range:
    start: "2022-01-01"
    end: "2024-01-01"
  frequency: "1h"  # Hourly

  # Market data
  market:
    exchange: "binance"
    fields: ["open", "high", "low", "close", "volume"]

  # On-chain data
  onchain:
    provider: "etherscan"  # or glassnode, dune
    metrics:
      - "active_addresses"
      - "transfer_volume"
      - "gas_fees"
      - "exchange_inflow"
      - "exchange_outflow"

  # Sentiment data
  sentiment:
    model: "ProsusAI/finbert"
    keywords:
      - "$USDT"
      - "$USDC"
      - "$DAI"
      - "stablecoin"
      - "depeg"
      - "tether"
      - "usdc"
    max_tweets_per_day: 10000
    max_reddit_posts_per_day: 1000

  # Output paths
  output_dir: "data/raw"
  processed_dir: "data/processed"

# --- Preprocessing ---
preprocessing:
  missing_values:
    short_gaps: "linear_interpolation"
    long_gaps: "forward_fill"
    max_gap_hours: 6
  outlier_method: "iqr"
  iqr_multiplier: 1.5
  normalization: "minmax"  # minmax or standard

# --- Feature Engineering ---
features:
  log_returns: true
  technical_indicators:
    - rsi:
        period: 14
    - macd:
        fast: 12
        slow: 26
        signal: 9
    - bollinger:
        period: 20
        std: 2
  lags: [1, 2, 3, 6, 12, 24]
  sentiment_aggregation: "hourly_mean"

# --- Model Architecture ---
model:
  name: "Deep-XAI-Stable"
  sequence_length: 48  # 48 hours lookback
  prediction_horizon: 1  # 1 hour ahead

  # Feature extraction
  feature_extractor:
    dense_units: [128, 64]
    activation: "relu"

  # LSTM layers
  lstm:
    num_layers: 2
    units: [128, 64]
    bidirectional: true
    return_sequences: true

  # Attention mechanism
  attention:
    type: "self"  # self or bahdanau
    units: 64

  # Output layers
  output:
    dense_units: [32, 16]
    activation: "relu"
    final_activation: "linear"

  # Regularization
  dropout: 0.2
  recurrent_dropout: 0.1

# --- Training ---
training:
  epochs: 200
  batch_size: 64
  optimizer:
    name: "adam"
    learning_rate: 0.001
    weight_decay: 0.0001
  loss: "mse"
  early_stopping:
    patience: 10
    min_delta: 0.0001
    restore_best_weights: true
  lr_scheduler:
    name: "reduce_on_plateau"
    factor: 0.5
    patience: 5

# --- Data Split ---
split:
  train: 0.70
  validation: 0.15
  test: 0.15
  method: "chronological"

# --- Evaluation ---
evaluation:
  metrics: ["rmse", "mae", "mape", "directional_accuracy"]
  baseline_models: ["arima", "simple_lstm", "gru"]
  diebold_mariano_test: true

# --- XAI ---
xai:
  method: "shap"
  explainer: "deep"  # deep or kernel
  num_background_samples: 100
  plots:
    - "feature_importance"
    - "dependence"
    - "local_explanation"

# --- Paths ---
paths:
  raw_data: "data/raw"
  processed_data: "data/processed"
  models: "outputs/models"
  results: "outputs/results"
  plots: "outputs/plots"
  logs: "outputs/logs"
